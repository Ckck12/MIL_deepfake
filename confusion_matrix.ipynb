{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     40_fake_video_fake_audio\n",
      "5                      34_real\n",
      "8                      38_real\n",
      "14                     23_real\n",
      "17                     43_real\n",
      "18                     25_real\n",
      "19                      1_real\n",
      "21                     35_real\n",
      "22                     39_real\n",
      "27                     30_real\n",
      "28                     36_real\n",
      "29                      6_real\n",
      "31                     22_real\n",
      "33    49_fake_video_fake_audio\n",
      "34                     31_real\n",
      "35                      3_real\n",
      "36                     13_real\n",
      "44                     28_real\n",
      "45                     42_real\n",
      "47                     46_real\n",
      "48                     44_real\n",
      "Name: video_name, dtype: object\n",
      "21\n",
      "Accuracy: 0.5882352941176471\n",
      "AUC: 0.4769736842105263\n",
      "F1 Score: 0.7407407407407408\n",
      "Recall: 0.9375\n",
      "Precision: 0.6122448979591837\n",
      "Confusion Matrix:\n",
      "[[ 0 19]\n",
      " [ 2 30]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"20240601_022007_results.csv\")\n",
    "\n",
    "# Extract predictions and labels\n",
    "y_true = df['label']\n",
    "y_pred = df['prediction']\n",
    "y_scores = df['confidence']  # These are the confidence scores, used for AUC\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 잘못 예측한 video_name을 출력\n",
    "wrong_predictions = df[df['label'] != df['prediction']]\n",
    "print(wrong_predictions['video_name'])\n",
    "print(len(wrong_predictions))\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
